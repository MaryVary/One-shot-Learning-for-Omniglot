{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot Learnig for Omniglot Dataset\n",
    "\n",
    "I reviewed literature to find references for best practices for one-shot learning and a verified architecture for Omniglot database. I found papers such as [FaceNet](https://arxiv.org/pdf/1503.03832.pdf) and [Siamese Neural Networks for One-Shot Image Recognition](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf).  \n",
    "As shown in the references, architecture based on Convolutional Neural Network and similarity function in the final layers work the best for training. Once the model is trained, a forward calculation is used to encode images and find the distance between two images. The probability found through sigmoid function can be used as a metric to compare two images. \n",
    "\n",
    "The model described in the second reference is implemented in below and the results look promising:\n",
    "\n",
    "https://sorenbouma.github.io/blog/oneshot/\n",
    "\n",
    "https://github.com/Goldesel23/Siamese-Networks-for-One-Shot-Learning\n",
    "\n",
    "Since I do not have a cloud account, I was not able to run the models and verify the results though.\n",
    "\n",
    "Below is the model architecture with two inputs.\n",
    "\n",
    "![alt text](Siamese_diagram_2.png \"Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "The model in defined in below using the architecture and initial parameters used in the [paper](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38951745"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (105, 105, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(7,7),activation='relu',\n",
    "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "#layer to merge two encoded inputs with the l1 distance between them\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#call this layer on list of two input tensors.\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "optimizer = Adam(0.00006)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Path \n",
    "The path will be used to upload images for training the model and also for applyting the model for one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/mvariani/Desktop/Projects/Courses/Fellowship/Omniglot\" #CHANGE THIS - path where the piPATH = \"C:/Users/mvariani/Desktop/Projects/Courses/Fellowship/Omniglot\" #CHANGE THIS - path where the pickled data is storedckled data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "The handwritten data is pickled as an N_classes x n_examples x width x height array (using load_data.py uploaded seperately), and there is an accompanyng dictionary to specify which indexes belong to which languages. The pickle for \"Train\" data is genereated from \"images_background\" and \"val\" from \"images_evaluation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training alphabets\n",
      "dict_keys(['Alphabet_of_the_Magi', 'Anglo-Saxon_Futhorc', 'Arcadian', 'Armenian', 'Asomtavruli_(Georgian)', 'Balinese', 'Bengali', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Braille', 'Burmese_(Myanmar)', 'Cyrillic', 'Early_Aramaic', 'Futurama', 'Grantha', 'Greek', 'Gujarati', 'Hebrew', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Japanese_(hiragana)', 'Japanese_(katakana)', 'Korean', 'Latin', 'Malay_(Jawi_-_Arabic)', 'Mkhedruli_(Georgian)', 'N_Ko', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Sanskrit', 'Syriac_(Estrangelo)', 'Tagalog', 'Tifinagh'])\n",
      "validation alphabets:\n",
      "dict_keys(['Angelic', 'Atemayar_Qelisayer', 'Atlantean', 'Aurek-Besh', 'Avesta', 'Ge_ez', 'Glagolitic', 'Gurmukhi', 'Kannada', 'Keble', 'Malayalam', 'Manipuri', 'Mongolian', 'Old_Church_Slavonic_(Cyrillic)', 'Oriya', 'Sylheti', 'Syriac_(Serto)', 'Tengwar', 'Tibetan', 'ULOG'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(PATH + \"/train.pickle\", \"rb\") as f:\n",
    "    (X,c) = pickle.load(f)\n",
    "\n",
    "with open(PATH +  \"/val.pickle\", \"rb\") as f:\n",
    "    (Xval,cval) = pickle.load(f)\n",
    "    \n",
    "print(\"training alphabets\")\n",
    "print(c.keys())\n",
    "print(\"validation alphabets:\")\n",
    "print(cval.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data and Test Files\n",
    "Below codes are used to generate batch files from \"train\" data sets for optimization. In order to evaluate the training, random set of data is generated from \"val\" data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from C:/Users/mvariani/Desktop/Projects/Courses/Fellowship/Omniglot\\train.pickle\n",
      "loading data from C:/Users/mvariani/Desktop/Projects/Courses/Fellowship/Omniglot\\val.pickle\n"
     ]
    }
   ],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n",
    "        self.data = {}\n",
    "        self.categories = {}\n",
    "        self.info = {}\n",
    "        \n",
    "        for name in data_subsets:\n",
    "            file_path = os.path.join(path, name + \".pickle\")\n",
    "            print(\"loading data from {}\".format(file_path))\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (X,c) = pickle.load(f)\n",
    "                self.data[name] = X\n",
    "                self.categories[name] = c\n",
    "\n",
    "    def get_batch(self,batch_size,s=\"train\"):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "        #randomly sample several classes to use in the batch\n",
    "        categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "        #initialize 2 empty arrays for the input image batch\n",
    "        pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "        #initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "        targets=np.zeros((batch_size,))\n",
    "        targets[batch_size//2:] = 1\n",
    "        for i in range(batch_size):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0, n_examples)\n",
    "            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "            idx_2 = rng.randint(0, n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            if i >= batch_size // 2:\n",
    "                category_2 = category  \n",
    "            else: \n",
    "                #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "                # ..different category\n",
    "                category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        return pairs, targets\n",
    "    \n",
    "    def generate(self, batch_size, s=\"train\"):\n",
    "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "        while True:\n",
    "            pairs, targets = self.get_batch(batch_size,s)\n",
    "            yield (pairs, targets)    \n",
    "\n",
    "    def make_oneshot_task(self,N,s=\"val\",language=None):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h = X.shape\n",
    "        indices = rng.randint(0,n_examples,size=(N,))\n",
    "        if language is not None:\n",
    "            low, high = self.categories[s][language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "            \n",
    "        else:#if no language specified just pick a bunch of random letters\n",
    "            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "        support_set = X[categories,indices,:,:]\n",
    "        support_set[0,:,:] = X[true_category,ex2]\n",
    "        support_set = support_set.reshape(N, w, h,1)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} random {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N,s)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "    \n",
    "    def train(self, model, epochs, verbosity):\n",
    "        model.fit_generator(self.generate(batch_size),\n",
    "                            \n",
    "                             )\n",
    "    \n",
    "    \n",
    "#Instantiate the class\n",
    "loader = Siamese_Loader(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Below loop is used for training and finding the best weights. As I mentioned, since I do not have a cloud account, I was not able to train the model for large number of iterations. Pelase note that the number of iterations is set to 2 to make sure code is running without errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "4.523834\n",
      "evaluating\n",
      "Evaluating model on 25 random 20 way one-shot learning tasks ...\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "print(\"!\")\n",
    "evaluate_every = 1 # interval for evaluating on one-shot tasks\n",
    "loss_every=50 # interval for printing loss (iterations)\n",
    "batch_size = 32\n",
    "# n_iter = 90000 #orig\n",
    "n_iter = 2\n",
    "N_way = 20 # how many classes for testing one-shot tasks>\n",
    "# n_val = 250 #how mahy one-shot tasks to validate on?\n",
    "n_val = 250 #how mahy one-shot tasks to validate on? orig\n",
    "best = -1\n",
    "weights_path = os.path.join(PATH, \"weights\")\n",
    "print(\"training\")\n",
    "for i in range(1, n_iter):\n",
    "    (inputs,targets)=loader.get_batch(batch_size)\n",
    "    loss=siamese_net.train_on_batch(inputs,targets)\n",
    "    print(loss)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"evaluating\")\n",
    "        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n",
    "        if val_acc >= best:\n",
    "            print(\"saving\")\n",
    "            siamese_net.save(weights_path)\n",
    "            best=val_acc\n",
    "\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model\n",
    "Once the model is trained, it can be used for one-shot learning purposes. Here \"allrun\" files are used to find the accuracy of the model. In each run iteration, the model.predit is used to find the probabilty based on distance of encoded images. Each image in the \"test\" folder is compared with the ones in \"train\" folder. The highest probabilty is used to label the image. The errors could be very large since the model is not trained with adequate number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification demo with Siamese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvariani\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:62: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " run 1 (error 90.0%)\n",
      " run 2 (error 85.0%)\n",
      " run 3 (error 90.0%)\n",
      " run 4 (error 65.0%)\n",
      " run 5 (error 60.0%)\n",
      " run 6 (error 90.0%)\n",
      " run 7 (error 85.0%)\n",
      " run 8 (error 85.0%)\n",
      " run 9 (error 80.0%)\n",
      " run 10 (error 85.0%)\n",
      " run 11 (error 80.0%)\n",
      " run 12 (error 90.0%)\n",
      " run 13 (error 85.0%)\n",
      " run 14 (error 90.0%)\n",
      " run 15 (error 70.0%)\n",
      " run 16 (error 80.0%)\n",
      " run 17 (error 85.0%)\n",
      " run 18 (error 65.0%)\n",
      " run 19 (error 95.0%)\n",
      " run 20 (error 90.0%)\n",
      " average error 82.25%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy.ndimage import imread\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#Loading the model\n",
    "#siamese_net = load_model(PATH + '/siamese_net.h5')\n",
    "\n",
    "# Parameters\n",
    "nrun = 20 # number of classification runs\n",
    "fname_label = 'class_labels.txt' # where class labels are stored for each run\n",
    "\n",
    "def classification_run(folder,f_load,siamese_prob,fpath):\n",
    "\n",
    "#     assert ((ftype=='cost') | (ftype=='score'))\n",
    "\n",
    "    # get file names\n",
    "    \n",
    "    with open(folder+'/'+fname_label) as f:\n",
    "        content = f.read().splitlines()\n",
    "    pairs = [line.split() for line in content]\n",
    "    test_files  = [pair[0] for pair in pairs]\n",
    "    train_files = [pair[1] for pair in pairs]\n",
    "    answers_files = copy.copy(train_files)\n",
    "    test_files.sort()\n",
    "    train_files.sort()\t\n",
    "    ntrain = len(train_files)\n",
    "    ntest = len(test_files)\n",
    "\n",
    "    # load the images (and, if needed, extract features)\n",
    "    train_items = [f_load(os.path.join(fpath,f)) for f in train_files]\n",
    "    test_items  = [f_load(os.path.join(fpath,f)) for f in test_files ]\n",
    "\n",
    "    # compute Probability matrix\n",
    "    ProbM = np.zeros((ntest,ntrain),float)\n",
    "    for i in range(ntest):\n",
    "        for c in range(ntrain):\n",
    "            ProbM[i,c] = siamese_prob(test_items[i],train_items[c])    \n",
    "   \n",
    "    YHAT = np.argmax(ProbM,axis=1)\n",
    "    \n",
    "\n",
    "    # compute the error rate\n",
    "    correct = 0\n",
    "    for i in range(ntest):\n",
    "        if train_files[YHAT[i]] == answers_files[i]:\n",
    "            correct += 1.0\n",
    "    pcorrect = 100 * correct / ntest\n",
    "    perror = 100 - pcorrect\n",
    "    return perror\n",
    "\n",
    "def Prob_Calc(itemA,itemB):\n",
    "    b1 = itemA.reshape(1,105,105,1)\n",
    "    b2 = itemB.reshape(1,105,105,1)\n",
    "    b = np.stack((b1, b2))\n",
    "    b = list(b)\n",
    "#     print(np.array(b).shape)\n",
    "    p=siamese_net.predict(b)\n",
    "    return p\n",
    "\n",
    "def LoadImgAsPoints(fn):\n",
    "    I = imread(fn,flatten=True)\n",
    "    return I\n",
    "\n",
    "f_path = PATH + '/all_runs/'\n",
    "print('One-shot classification demo with Siamese')\n",
    "perror = np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "    rs = str(r)\n",
    "    if len(rs)==1:\n",
    "        rs = '0' + rs\n",
    "    perror[r-1] = classification_run(f_path+'run'+rs, LoadImgAsPoints, Prob_Calc,f_path)\n",
    "    print(\" run \" + str(r) + \" (error \" + str(perror[r-1] ) + \"%)\")\n",
    "total = np.mean(perror)\n",
    "print(\" average error \" + str(total) + \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
